{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_dataset.csv')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def premium_boost(x):\n",
    "    return 2 if (x['user_premium']) else 1\n",
    "\n",
    "def clicked(x):\n",
    "    return x['interaction_type'] == 1\n",
    "\n",
    "def bookmarked(x):\n",
    "    return x['interaction_type'] == 2\n",
    "\n",
    "def replied(x):\n",
    "    return x['interaction_type'] == 3\n",
    "\n",
    "def recruiter_interest(x):\n",
    "    return x['interaction_type'] == 5\n",
    "\n",
    "def deleted(x):\n",
    "    return x['interaction_type'] == 4\n",
    "\n",
    "def user_success(x):\n",
    "    return premium_boost(x) * (1 if clicked(x) else 0 + 5 if (bookmarked(x) or replied(x)) else 0 + 20 if recruiter_interest(x) else 0 - 10 if deleted else 0)\n",
    "\n",
    "def item_success(x):\n",
    "    return 5 if x[\"item_is_payed\"] else 0\n",
    "\n",
    "def score(x):\n",
    "    return item_success(x) + user_success(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = df.apply(score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scores.mean())\n",
    "#print(scores.max())\n",
    "#print(scores.min())\n",
    "#scores.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_country'] = df['user_country'].astype('category').cat.rename_categories([0,1,2,3])\n",
    "df['item_country'] = df['item_country'].astype('category').cat.rename_categories([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_hot_encode(column):\n",
    "    # get unique elements from df[column] and put them into a set\n",
    "    _set = set()\n",
    "\n",
    "    for _list in df[column]:\n",
    "        for _elem in _list:\n",
    "            _set.add(_elem)\n",
    "\n",
    "    # for each row, compute it's encoded vector indicating with a \n",
    "    # 1 that it contains that element from the set and a 0 that it doesn't\n",
    "    _vectors = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        _vectors.append([1 if _elem in df[column][i] else 0 for _elem in _set])\n",
    "\n",
    "    return _vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "p = Pool(4)\n",
    "results = p.map(n_hot_encode, [\n",
    "    'user_title',\n",
    "    'item_title',\n",
    "    'item_tags',\n",
    "    'user_edu_fieldofstudies'\n",
    "])\n",
    "df['nhot_user_title'] = results[0]\n",
    "df['nhot_item_title'] = results[1]\n",
    "df['nhot_item_tags'] = results[2]\n",
    "df['nhot_user_edu_fieldofstudies'] = results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.interaction_type.value_counts())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df with nhot encoded columns\n",
    "df.to_csv('nhot_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = np.zeros(shape=(26614314, 73))\n",
    "target = np.zeros(shape=(26614314,))\n",
    "next_index = 0\n",
    "chunk_size = 1000000\n",
    "\n",
    "for df_chunk in pd.read_csv('nhot_df.csv', chunksize=chunk_size):\n",
    "    columns = np.setdiff1d(df_chunk.columns.values, ['Unnamed: 0', 'user_title', 'item_title', 'user_edu_fieldofstudies', 'item_tags', 'nhot_user_title', 'nhot_item_title', 'nhot_user_edu_fieldofstudies', 'nhot_item_tags', 'interaction_type', 'item_created_at', 'interaction_created_at'])\n",
    "\n",
    "    def full_encoding(row):\n",
    "        regular_columns = [row[col] for col in columns]\n",
    "        nhot_columns = \\\n",
    "            ast.literal_eval(row['nhot_user_title']) + \\\n",
    "            ast.literal_eval(row['nhot_item_title']) + \\\n",
    "            ast.literal_eval(row['nhot_user_edu_fieldofstudies']) + \\\n",
    "            ast.literal_eval(row['nhot_item_tags'])\n",
    "        return regular_columns + nhot_columns\n",
    "\n",
    "    for index, row in df_chunk.iterrows():\n",
    "        data[next_index] = full_encoding(row)\n",
    "        target[next_index] = row['interaction_type']\n",
    "        next_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# columns = np.setdiff1d(df.columns.values, ['Unnamed: 0', 'user_title', 'item_title', 'user_edu_fieldofstudies', 'item_tags', 'nhot_user_title', 'nhot_item_title', 'nhot_user_edu_fieldofstudies', 'nhot_item_tags', 'interaction_type', 'item_created_at', 'interaction_created_at'])\n",
    "# columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def full_encoding(row):\n",
    "#     regular_columns = [row[col] for col in columns]\n",
    "#     nhot_columns = list(row['nhot_user_title']) + list(row['nhot_item_title']) + list(row['nhot_user_edu_fieldofstudies']) + list(row['nhot_item_tags'])\n",
    "#     return regular_columns + nhot_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_df = df.apply(full_encoding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.zeros(shape=(len(encoded_df), len(encoded_df[0])))\n",
    "\n",
    "# # fill array with data taken from the df\n",
    "# for i, row in enumerate(encoded_df):\n",
    "#     data[i] = np.asarray(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = df['interaction_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# save array to be used for training\n",
    "print(data.shape)\n",
    "\n",
    "save_x_file = open('data.npy', 'wb')\n",
    "np.save(save_x_file, data)\n",
    "\n",
    "save_y_file = open('target.npy', 'wb')\n",
    "np.save(save_y_file, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 140 ms, sys: 7.58 s, total: 7.72 s\n",
      "Wall time: 7.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load array for training\n",
    "import numpy as np\n",
    "\n",
    "save_x_file = open('data.npy', 'rb')\n",
    "data = np.load(save_x_file)\n",
    "\n",
    "save_y_file = open('target.npy', 'rb')\n",
    "target = np.load(save_y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/notebooks/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with nan values\n",
    "# NOT NECESSARY BECAUSE WE'RE NOT USING THE TIMESTAMPS WHICH CONTAINED nan VALUES\n",
    "\n",
    "#print(encoded_df.shape)\n",
    "#clean_df = encoded_df[np.isnan(data).any(axis=1)]\n",
    "#print(clean_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 20631504), (1.0, 4717347), (2.0, 264646), (3.0, 91526), (4.0, 906799), (5.0, 2492)]\n",
      "[(0.0, 2063150), (1.0, 4717347), (2.0, 264646), (3.0, 91526), (4.0, 90679), (5.0, 2492)]\n",
      "[(0.0, 4717347), (1.0, 4717347), (2.0, 4717347), (3.0, 4717347), (4.0, 4717347), (5.0, 4717347)]\n",
      "CPU times: user 42.3 s, sys: 8.24 s, total: 50.5 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# resample the data to get more balanced classes\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.datasets import make_imbalance\n",
    "from collections import Counter\n",
    "\n",
    "def ratio_multiplier(y):\n",
    "    multiplier = {0: 0.1, 1: 1, 2: 1, 3: 1, 4: 0.1, 5: 1}\n",
    "    target_stats = Counter(y)\n",
    "    for key, value in target_stats.items():\n",
    "        target_stats[key] = int(value * multiplier[key])\n",
    "    return target_stats\n",
    "\n",
    "print(sorted(Counter(target).items()))\n",
    "x_resampled, y_resampled = RandomUnderSampler(random_state=42, ratio=ratio_multiplier).fit_sample(data, target)\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "x_resampled, y_resampled = RandomOverSampler(random_state=42).fit_sample(x_resampled, y_resampled)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_resampled = to_categorical(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_resampled, \n",
    "    y_resampled, \n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_resampled.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, activation='relu', input_dim=input_dim))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation='relu', input_dim=input_dim))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_resampled.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_resampled.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, activation='relu', input_dim=input_dim))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation='relu', input_dim=input_dim))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(y_resampled.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a callback to stop if we start overfitting\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "callbacks = [EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=1, verbose=0, mode='auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1024,1024\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    validation_data=(x_test, y_test), \n",
    "    epochs=20, \n",
    "    batch_size=128,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22643265 samples, validate on 5660817 samples\n",
      "Epoch 1/20\n",
      "22643265/22643265 [==============================] - 1321s 58us/step - loss: 1.0439 - acc: 0.5707 - val_loss: 0.9080 - val_acc: 0.6187\n",
      "Epoch 2/20\n",
      "    1792/22643265 [..............................] - ETA: 5:27:23 - loss: 0.9862 - acc: 0.5876 "
     ]
    }
   ],
   "source": [
    "#512,512,256\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    validation_data=(x_test, y_test), \n",
    "    epochs=20, \n",
    "    batch_size=128,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, label='training loss')\n",
    "plt.plot(epochs, val_loss, label='validation loss')\n",
    "plt.plot(epochs, acc, label='training acc')\n",
    "plt.plot(epochs, val_acc, label='validation acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = np.expand_dims(data[0], axis=0)  # predict one instance\n",
    "print(model.predict(x_test[:2]))  # predict some instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.predict(x_test[1:2])[0]\n",
    "print(arr.max(), np.argmax(arr), arr.min(), np.argmin(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(predictions, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
